{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSL & Contrastive Learning Experiment Runner\n",
    "\n",
    "This notebook serves as the main entry point for running all experiments and analyzing the results. It will execute the Python scripts in the `../scripts/` directory to train each model sequentially. \n",
    "\n",
    "The process is:\n",
    "1.  **Part 1:** Train the **Supervised Baseline** model.\n",
    "2.  **Part 2:** Train the **Self-Supervised (Pretext Task)** model (pre-training + fine-tuning).\n",
    "3.  **Part 3:** Train the **Contrastive (SimSiam)** model (pre-training + linear probing).\n",
    "4.  **Part 4:** Load the saved history files (`.json`) from all runs and generate **comparative plots**.\n",
    "5.  **Part 5:** Load the pre-trained SimSiam backbone and visualize the learned feature space using **UMAP**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Train Supervised Baseline\n",
    "\n",
    "This script trains a standard CNN from scratch on only 5,000 labeled images. We expect this to overfit and perform poorly, establishing our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!python ../scripts/train_baseline.py --epochs 50 --subset_size 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Train Self-Supervised (Pretext Task)\n",
    "\n",
    "This script first pre-trains the model on all 50,000 images using a multi-task pretext objective (predicting rotation, shear, and color augmentations). \n",
    "\n",
    "After pre-training, it fine-tunes the *entire* network on the small 5,000-image labeled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../scripts/train_ssl_pretext.py --pretext_epochs 30 --finetune_epochs 30 --subset_size 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Train Contrastive (SimSiam)\n",
    "\n",
    "This script first pre-trains a SimSiam model on all 50,000 unlabeled images. \n",
    "\n",
    "After pre-training, it performs **linear probing**: the backbone is *frozen*, and only a newly attached linear classifier is trained on the 5,000-image labeled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../scripts/train_siam.py --pretrain_epochs 30 --probe_epochs 50 --subset_size 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Comparative Analysis\n",
    "\n",
    "Now we load the history files saved by our scripts to compare the performance of all three methods on the same test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "HISTORY_DIR = \"../outputs\"\n",
    "\n",
    "try:\n",
    "    with open(os.path.join(HISTORY_DIR, \"baseline_history.json\"), 'r') as f:\n",
    "        history_baseline = json.load(f)\n",
    "\n",
    "    with open(os.path.join(HISTORY_DIR, \"ssl_finetune_history.json\"), 'r') as f:\n",
    "        history_ssl = json.load(f)\n",
    "\n",
    "    with open(os.path.join(HISTORY_DIR, \"simsiam_ft_history.json\"), 'r') as f:\n",
    "        history_siam = json.load(f)\n",
    "\n",
    "    print(\"Successfully loaded all history files.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}. Please ensure all training scripts have been run successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the final comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "fig.suptitle('Model Performance Comparison (Trained on 5k Labeled Images)', fontsize=18)\n",
    "\n",
    "# --- Loss Plot ---\n",
    "ax1.plot(history_baseline['test_loss'], label='Baseline Test Loss', linestyle='-')\n",
    "ax1.plot(history_ssl['test_loss'], label='SSL (Pretext) Test Loss', linestyle='--')\n",
    "ax1.plot(history_siam['test_loss'], label='SimSiam (Probe) Test Loss', linestyle=':')\n",
    "ax1.set_title('Test Loss Comparison')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# --- Accuracy Plot ---\n",
    "ax2.plot(history_baseline['test_acc'], label=f\"Baseline Test Acc (Max: {max(history_baseline['test_acc']):.2f}%)\", linestyle='-')\n",
    "ax2.plot(history_ssl['test_acc'], label=f\"SSL (Pretext) Test Acc (Max: {max(history_ssl['test_acc']):.2f}%)\", linestyle='--')\n",
    "ax2.plot(history_siam['test_acc'], label=f\"SimSiam (Probe) Test Acc (Max: {max(history_siam['test_acc']):.2f}%)\", linestyle=':')\n",
    "ax2.set_title('Test Accuracy Comparison')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.savefig(os.path.join(HISTORY_DIR, \"final_comparison.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Results\n",
    "\n",
    "From the plots, we can draw several key conclusions:\n",
    "\n",
    "1.  **Baseline:** The supervised baseline model, trained only on 5,000 images, performs the worst. It overfits quickly (as seen in its individual training logs) and its test accuracy plateaus at the lowest level, around **~63-65%**. This confirms our hypothesis that supervised learning is insufficient in this low-data regime.\n",
    "\n",
    "2.  **Self-Supervised (Pretext):** The pretext-task model performs noticeably better, achieving a peak accuracy of **~67-68%**. The pre-training on all 50,000 images, even with a simple task like predicting augmentations, allows the backbone to learn more robust and generalizable features. This provides a much better starting point for fine-tuning on the small labeled set.\n",
    "\n",
    "3.  **SimSiam (Linear Probe):** The SimSiam model, which uses a more advanced contrastive learning objective, shows the most stable performance. By freezing the backbone and training only the linear classifier, it is highly resistant to overfitting on the small 5k dataset. While its peak accuracy (**~65-66%**) is slightly lower than the fully fine-tuned SSL model, it achieves this by training *far fewer* parameters. This demonstrates that the SimSiam pre-training successfully learned a rich, linearly separable representation of the data. The UMAP plot below will further confirm this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Feature Visualization (UMAP)\n",
    "\n",
    "Finally, we visualize the feature space learned by the **SimSiam backbone**. We load the pre-trained `simsiam_model.pth`, extract features (the 512-dim *predictions*) for all test images, and plot them using UMAP, colored by their true class labels. \n",
    "\n",
    "If the pre-training was successful, we should see clear clusters of a single color, indicating that the model has learned to group images of the same class together *without ever having seen their labels*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "import umap.umap_ as umap\n",
    "\n",
    "# Adjust path to import from src\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), '..')))\n",
    "\n",
    "from src.models import SimSiam\n",
    "from src.data_loader import get_baseline_loaders\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 1. Load the pre-trained SimSiam model\n",
    "try:\n",
    "    simsiam = SimSiam().to(device)\n",
    "    simsiam.load_state_dict(torch.load('../outputs/simsiam_model.pth', map_location=device))\n",
    "    simsiam.eval()\n",
    "    print(\"SimSiam model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load model: {e}\")\n",
    "\n",
    "# 2. Get the test loader (with standard transforms)\n",
    "_, test_loader, _, _ = get_baseline_loaders(batch_size=128, subset_size=1) # subset_size doesn't matter here\n",
    "\n",
    "# 3. Extract features (predictions) for all test images\n",
    "features = []\n",
    "labels = []\n",
    "print(\"Extracting features from test set...\")\n",
    "with torch.no_grad():\n",
    "    for images, target in tqdm(test_loader, total=len(test_loader)):\n",
    "        images = images.to(device)\n",
    "        proj, pred = simsiam(images)\n",
    "        \n",
    "        features.extend(pred.detach().cpu().numpy())\n",
    "        labels.extend(target.detach().cpu().numpy())\n",
    "\n",
    "features = np.array(features)\n",
    "labels = [str(l) for l in labels] # For discrete colors in plotly\n",
    "print(f\"Extracted {features.shape[0]} features.\")\n",
    "\n",
    "# 4. Run UMAP\n",
    "print(\"Running UMAP... (This may take a moment)\")\n",
    "reducer = umap.UMAP(n_components=2, n_neighbors=15, metric=\"cosine\")\n",
    "projections = reducer.fit_transform(features)\n",
    "\n",
    "# 5. Plot with Plotly\n",
    "print(\"Generating plot...\")\n",
    "fig = px.scatter(projections, x=0, y=1,\n",
    "                 color=labels, labels={'color': 'Cifar10 Labels'},\n",
    "                 title=\"UMAP Projection of SimSiam Features (Test Set)\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP Analysis\n",
    "\n",
    "The UMAP plot visualizes the high-dimensional feature space in 2D. As we can see, the model has formed distinct clusters that correspond well to the true class labels. For example, you can clearly see separate groupings for different classes.\n",
    "\n",
    "This is a powerful result: it confirms that the SimSiam backbone, *without using any labels*, has learned a semantically meaningful representation of the data. The classes are already well-separated in this feature space, which is why a simple linear classifier (our probe) was able to perform so well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
